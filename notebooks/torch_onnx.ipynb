{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name           target                                                args                kwargs\n",
      "-------------  -------------  ----------------------------------------------------  ------------------  -----------\n",
      "placeholder    x              x                                                     ()                  {}\n",
      "get_attr       linear_weight  linear.weight                                         ()                  {}\n",
      "call_function  add            <built-in function add>                               (x, linear_weight)  {}\n",
      "call_module    linear         linear                                                (add,)              {}\n",
      "call_method    relu           relu                                                  (linear,)           {}\n",
      "call_function  sum_1          <built-in method sum of type object at 0x10bd77788>   (relu,)             {'dim': -1}\n",
      "call_function  topk           <built-in method topk of type object at 0x10bd77788>  (sum_1, 3)          {}\n",
      "output         output         output                                                (topk,)             {}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.fx\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.Parameter(torch.rand(3, 4))\n",
    "        self.linear = torch.nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.topk(torch.sum(\n",
    "            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n",
    "\n",
    "m = MyModule()\n",
    "gm = torch.fx.symbolic_trace(m)\n",
    "\n",
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_36857/731836689.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img = torch.tensor(torch.arange(3 * 9 * 9).reshape(3,9,9),requires_grad=True,dtype=torch.float32)\n",
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_36857/731836689.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ker = torch.tensor(torch.arange(3 * 3 * 3).reshape(1,3,3,3),requires_grad=True,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "img = torch.tensor(torch.arange(3 * 9 * 9).reshape(3,9,9),requires_grad=True,dtype=torch.float32)\n",
    "ker = torch.tensor(torch.arange(3 * 3 * 3).reshape(1,3,3,3),requires_grad=True,dtype=torch.float32)\n",
    "fet = torch.conv2d(img,ker,stride=2,dilation=1)\n",
    "x = fet.sum(0).sum(0).sum(0)\n",
    "# gm = torch.fx.symbolic_trace(x)\n",
    "# gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_36857/2941385597.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ker = torch.tensor(torch.arange(3 * 3 * 3).reshape(1,3,3,3),requires_grad=True,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "class MySingleConvolution(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ker = torch.tensor(torch.arange(3 * 3 * 3).reshape(1,3,3,3),requires_grad=True,dtype=torch.float32)\n",
    "        self.ker = torch.nn.Parameter(ker)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # img = torch.tensor(torch.arange(3 * 9 * 9).reshape(3,9,9),requires_grad=True,dtype=torch.float32)\n",
    "        fet = torch.conv2d(img,self.ker,stride=2,dilation=1)\n",
    "        return fet\n",
    "\n",
    "my_conv = MySingleConvolution()\n",
    "\n",
    "from torch.fx import symbolic_trace\n",
    "trace = symbolic_trace(my_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %img : [num_users=1] = placeholder[target=img]\n",
      "    %ker : [num_users=1] = get_attr[target=ker]\n",
      "    %conv2d : [num_users=1] = call_function[target=torch.conv2d](args = (%img, %ker), kwargs = {stride: 2, dilation: 1})\n",
      "    return conv2d\n"
     ]
    }
   ],
   "source": [
    "print(trace.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_36857/370995943.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ker = torch.tensor(torch.arange(4 * 3 * 3 * 3).reshape(4,3,3,3),dtype=torch.float32)\n",
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_36857/370995943.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dummy_input = torch.tensor(torch.arange(3 * 9 * 9).reshape(3,9,9),requires_grad=False,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "class MySingleConvolution(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ker = torch.tensor(torch.arange(4 * 3 * 3 * 3).reshape(4,3,3,3),dtype=torch.float32)\n",
    "        self.ker = torch.nn.Parameter(ker)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # img = torch.tensor(torch.arange(3 * 9 * 9).reshape(3,9,9),requires_grad=True,dtype=torch.float32)\n",
    "        fet = torch.conv2d(img,self.ker,stride=1,dilation=1)\n",
    "        return fet\n",
    "\n",
    "my_conv = MySingleConvolution()\n",
    "\n",
    "dummy_input = torch.tensor(torch.arange(3 * 9 * 9).reshape(3,9,9),requires_grad=False,dtype=torch.float32)\n",
    "my_conv(dummy_input)\n",
    "\n",
    "input_names = ['image_batch', 'kernel_weights']\n",
    "output_names = ['features_batch']\n",
    "torch.onnx.export(my_conv, dummy_input, 'MySingleConv.onnx', verbose=False, input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelProto {\n",
      "  producer_name: \"pytorch\"\n",
      "  domain: \"\"\n",
      "  doc_string: \"\"\n",
      "  graph:\n",
      "    GraphProto {\n",
      "      name: \"main_graph\"\n",
      "      inputs: [{name: \"image_batch\", type:Tensor dtype: 1, Tensor dims: 3 9 9},{name: \"kernel_weights\", type:Tensor dtype: 1, Tensor dims: 4 3 3 3}]\n",
      "      outputs: [{name: \"features_batch\", type:Tensor dtype: 1, Tensor dims: 4 7 7}]\n",
      "      value_infos: []\n",
      "      initializers: [TensorProto shape: [4 3 3 3]]\n",
      "      nodes: [\n",
      "        Node {type: \"Constant\", inputs: [], outputs: [onnx::Unsqueeze_2], attributes: [{ name: 'value', type: tensor, value:TensorProto shape: [1]}]},\n",
      "        Node {type: \"Unsqueeze\", inputs: [image_batch,onnx::Unsqueeze_2], outputs: [onnx::Conv_3], attributes: []},\n",
      "        Node {type: \"Conv\", inputs: [onnx::Conv_3,kernel_weights], outputs: [onnx::Squeeze_4], attributes: [{ name: 'dilations', type: ints, values: [1 1]},{ name: 'group', type: int, value: 1},{ name: 'kernel_shape', type: ints, values: [3 3]},{ name: 'pads', type: ints, values: [0 0 0 0]},{ name: 'strides', type: ints, values: [1 1]}]},\n",
      "        Node {type: \"Constant\", inputs: [], outputs: [onnx::Squeeze_5], attributes: [{ name: 'value', type: tensor, value:TensorProto shape: [1]}]},\n",
      "        Node {type: \"Squeeze\", inputs: [onnx::Squeeze_4,onnx::Squeeze_5], outputs: [features_batch], attributes: []}\n",
      "      ]\n",
      "    }\n",
      "  opset_import: [OperatorSetIdProto { domain: , version: 17}],\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_36857/1459201870.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dummy_input = torch.tensor(torch.arange(3 * 9 * 9).reshape(3,9,9),requires_grad=True,dtype=torch.float32)\n",
      "/var/folders/8v/r6tfrh3s0y16r9dl8fzkgdgw0000gp/T/ipykernel_36857/370995943.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ker = torch.tensor(torch.arange(4 * 3 * 3 * 3).reshape(4,3,3,3),dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.tensor(torch.arange(3 * 9 * 9).reshape(3,9,9),requires_grad=True,dtype=torch.float32)\n",
    "my_conv = MySingleConvolution()\n",
    "my_conv(dummy_input)\n",
    "\n",
    "s = torch.onnx.export_to_pretty_string(my_conv, dummy_input,keep_initializers_as_inputs=True,export_params=True, verbose=True, input_names=input_names, output_names=output_names)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph main_graph (\n",
      "  %image_batch[FLOAT, 3x9x9]\n",
      ") initializers (\n",
      "  %kernel_weights[FLOAT, 4x3x3x3]\n",
      ") {\n",
      "  %/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Unsqueeze_output_0 = Unsqueeze(%image_batch, %/Constant_output_0)\n",
      "  %/Conv_output_0 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [1, 1]](%/Unsqueeze_output_0, %kernel_weights)\n",
      "  %/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %features_batch = Squeeze(%/Conv_output_0, %/Constant_1_output_0)\n",
      "  return %features_batch\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load('MySingleConv.onnx')\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
